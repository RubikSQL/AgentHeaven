core:
    env: dev
    debug: false
    encoding: utf-8
    http_proxy: null
    https_proxy: null
    tmp_path: "~/.ahvn/tmp/"
    cache_path: "~/.ahvn/cache/"
    encrypt_keys:
        - "api_key"
        - "token"
        - "password"
        - "url"

ukf:
    version: "1.0.0"
    text:
        id: 63
        short: 255
        medium: 2047
        long: 65535
    db:
        brief_enum_sample_top: 2
        brief_enum_sample_bottom: 1
        detail_enum_sample_top: 5
        detail_enum_sample_bottom: 5
        detail_col_sample_top: -1
        detail_col_sample_bottom: 2

klstore:
    batch_size: 512

klengine:
    sync_batch_size: 512

user:
    user_id: admin

prompts:
    langs:
        - en
        - zh
    main: en
    lang: zh
    scan:
        - "& prompts/"
        - "~/.ahvn/prompts/"

llm:
    litellm_debug: false
    # LiteLLM compatible format:
    #   <provider>/<model_identifier>
    default_args:
        seed: 42
        timeout: 120
        http_proxy: null
        https_proxy: null
        repair_tool_calls: true
        enforce_non_stream: false  # TODO: support inference backends that are not stream-compatible for tooluse
    
    default_preset: sys
    presets:
        sys:
            model: dsv3
        local:
            provider: ollama
            model: qwen3-30b
        tiny:
            provider: ollama
            model: qwen3-4b
        chat-expert:
            model: gpt-5.2
        chat:
            model: dsv3
        chat-local:
            provider: ollama
            model: qwen3-30b
        reason-expert:
            model: gemini-pro
        reason:
            model: dsr1
        reason-local:
            provider: ollama
            model: dsqw-32b
        coder-expert:
            model: opus
        coder:
            model: dsv3
        coder-local:
            provider: ollama
            model: qwen3-coder:30b
        translator:
            model: gemini-flash
        embedder-expert:
            provider: ollama
            model: qwen3-embedding:8b
        embedder:
            provider: ollama
            model: embeddinggemma
        embedder-tiny:
            provider: ollama
            model: all-minilm:33m

    default_model: dsv3
    models:
        GPT-5.2:
            aliases:
                - gpt5.2
                - GPT5.2
                - gpt-5.2
            default_provider: openrouter
            identifiers:
                openrouter: openai/gpt-5.2
                openai: gpt-5.2
        Gemini-3.0-Flash:
            aliases:
                - gemini-3.0-flash
                - gemini-flash
            default_provider: openrouter
            identifiers:
                openrouter: google/gemini-3-flash-preview
                gemini: gemini-3-flash-preview
            default_args:
                temperature: 1.0  # For Gemini 3 models, LiteLLM defaults temperature to 1.0 and strongly recommends keeping it at this default.
        Gemini-3.0-Pro:
            aliases:
                - gemini-3.0-pro
                - gemini-3.0-pro-preview
                - gemini-3-pro
                - gemini-3-pro-preview
                - gemini-pro
            default_provider: openrouter
            identifiers:
                openrouter: google/gemini-3-pro-preview
                gemini: gemini-3-pro-preview
            default_args:
                temperature: 1.0  # For Gemini 3 models, LiteLLM defaults temperature to 1.0 and strongly recommends keeping it at this default.
        Claude-Opus-4.5:
            aliases:
                - opus
                - opus4.5
                - Opus4.5
                - opus-4.5
                - Opus-4.5
                - claude
                - claude-opus
                - Claude-Opus
                - claude-opus-4-5-20251101
            default_provider: openrouter
            identifiers:
                openrouter: anthropic/claude-opus-4.5
                anthropic: claude-opus-4-5-20251101
        DeepSeek-V3.2:
            aliases:
                - ds
                - dsv3
                - v3.2
                - dsv3.2
                - dsv3.2:671b
                - dsv3.2-671b
                - DeepSeek-v3.2
                - deepseek-v3.2
                - deepseek-v3.2:671b
                - deepseek-chat
            default_provider: openrouter
            identifiers:
                openrouter: deepseek/deepseek-v3.2
                deepseek: deepseek-chat
        DeepSeek-V3.2-Thinking:
            aliases:
                - dsr1
                - v3.2-thinking
                - dsv3.2-thinking
                - dsv3.2-thinking:671b
                - dsv3.2-thinking-671b
                - DeepSeek-v3.2-Thinking
                - deepseek-v3.2-thinking
                - deepseek-v3.2-thinking:671b
                - deepseek-reasoner
            default_provider: openrouter
            identifiers:
                openrouter: deepseek/deepseek-v3.2
                deepseek: deepseek-reasoner
        DeepSeek-R1-Distilled-Qwen-32B:
            aliases:
                - dsqw
                - dsqw:32b
                - dsqw-32b
            default_provider: ollama
            identifiers:
                ollama: deepseek-r1:32b
                openrouter: deepseek/deepseek-r1-distill-qwen-32b
        Qwen3-Coder:
            aliases:
                - qwen3-coder
                - qwen3-coder:30b
                - qwen3-coder-30b
                - Qwen3-Coder-30B
            default_provider: ollama
            identifiers:
                ollama: qwen3-coder:30b
                openrouter: qwen/qwen3-coder
        Qwen3-30B-Instruct:
            aliases:
                - qwen-30b
                - qwen:30b
                - Qwen-30B
                - qwen3-30b
                - qwen3:30b
                - Qwen3-30B
                - Qwen3-30B-A3B
                - Qwen3-30B-A3B-Instruct-2507
            default_provider: ollama
            identifiers:
                ollama: qwen3:30b-a3b-instruct-2507-q4_K_M
                openrouter: qwen/qwen3-30b-a3b-instruct-2507
        Qwen3-4B-Instruct:
            aliases:
                - qwen3-4b
                - qwen3:4b
                - Qwen3-4B
                - Qwen3-4B-Instruct-2507
            default_provider: ollama
            identifiers:
                ollama: qwen3:4b
                lmstudio: qwen/qwen3-4b-2507
        all-MiniLM-L12-v2:
            aliases:
                - minilm
                - all-minilm
                - all-minilm:33m
                - all-MiniLM
            default_provider: ollama
            identifiers:
                ollama: all-minilm:33m
            dimension: 384
        EmbeddingGemma:
            aliases:
                - embeddinggemma
                - embedding-gemma
            default_provider: ollama
            identifiers:
                ollama: embeddinggemma
            dimension: 768
        Qwen3-Embedding-8B:
            aliases:
                - qwen3-embedding
                - qwen3-embedding:8b
                - qwen3-embedding-8b
            default_provider: ollama
            identifiers:
                ollama: qwen3-embedding:8b
            dimension: 4096

    default_provider: openrouter
    providers:
        openrouter:
            backend: "openrouter"
            api_key: "<OPENROUTER_API_KEY>"
            api_base: "https://openrouter.ai/api/v1"
        openai:
            backend: ""
            api_key: "<OPENAI_API_KEY>"
            api_base: "https://api.openai.com/v1"
        gemini:
            backend: "gemini"
            api_key: "<GEMINI_API_KEY>"
        anthropic:
            backend: "anthropic"
            api_key: "<ANTHROPIC_API_KEY>"
        deepseek:
            backend: "deepseek"
            api_key: "<DEEPSEEK_API_KEY>"
            api_base: "https://api.deepseek.com/beta"
        ollama:
            backend: "ollama"
        lmstudio:
            backend: "lm_studio"
            api_base: "http://localhost:1234/v1"
        vllm:
            backend: "hosted_vllm" # Self-hosted VLLM uses "hosted_vllm" as the backend
            api_base: "<VLLM_API_BASE>"

    handle_model_mismatch: ignore
    cache_exclude_keys:
        - "base_url"
        - "api_key"
        - "api_base"
        - "api_version"
        - "timeout"
        - "num_retries"
        - "metadata"
        - "stream_options"
        - "logit_bias"
        - "log_probs"
        - "top_logprobs"
        - "input_cost_per_token"
        - "output_cost_per_token"
        - "model_list"
    retry:
        max_attempts: 3
        multiplier: 1
        max: 60
        reraise: True

db:
    # SQLAlchemy compatible format:
    #   <dialect>+<driver>://<username>:<password>@<host>:<port>/<database>
    default_provider: sqlite
    display:
        max_rows: 64
        max_width: 64
        style: "DEFAULT"
    providers:
        sqlite:
            dialect: sqlite
            database: "./.ahvn/sqlite.db"
            # Pool: StaticPool for file DBs, SingletonThreadPool for :memory:
            pool:
                pool_pre_ping: false
                pool_recycle: -1
        duckdb:
            dialect: duckdb
            database: "./.ahvn/duckdb.db"
            # Pool: NullPool (creates new connection each time, thread-safe)
            pool:
                pool_pre_ping: false
                pool_recycle: -1
        pg:
            dialect: postgresql
            driver: psycopg2
            host: "localhost"
            port: 5432
            username: "${whoami}"
            # Pool: QueuePool with pre-ping for stale connection detection
            pool:
                pool_pre_ping: true
                pool_size: 5
                max_overflow: 10
                pool_timeout: 30
                pool_recycle: 3600
        mysql:
            dialect: mysql
            driver: pymysql
            host: "localhost"
            port: 3306
            username: "root"
            # Pool: Conservative settings to avoid "gone away" errors
            pool:
                pool_pre_ping: true
                pool_size: 3
                max_overflow: 2
                pool_timeout: 30
                pool_recycle: 1800
        mssql:
            dialect: mssql
            driver: pyodbc
            host: "localhost"
            port: 1433
            username: "sa"
            query_params:
                driver: "ODBC Driver 18 for SQL Server"
                TrustServerCertificate: "yes"
            # Pool: Similar to PostgreSQL
            pool:
                pool_pre_ping: true
                pool_size: 5
                max_overflow: 10
                pool_timeout: 30
                pool_recycle: 3600

vdb:
    # Supports Lance, ChromaDB, Milvus, and PGVector
    default_provider: lancedb
    default_embedder: embedder
    providers:
        simple:
            backend: simple
        lancedb:
            backend: lancedb
            uri: "./.ahvn/lancedb/"
            collection: "default"
            refine_factor: 10
        chromalite:
            backend: chroma
            mode: "ephemeral"
            collection: "default"
        chroma:
            backend: chroma
            mode: "persistent"
            path: "./.ahvn/chromadb/"
            collection: "default"
        milvuslite:
            backend: milvus
            uri: "./.ahvn/milvus.db"
            collection: "default"
        milvus:
            backend: milvus
            uri: "http://localhost:19530"
            db_name: "default"
            collection: "default"
        pgvector:
            backend: pgvector
            dialect: postgresql
            host: "localhost"
            port: 5432
            username: "${whoami}"
            collection: "default"

mdb:
    # MongoDB configuration (Community Edition)
    default_embedder: embedder  # for vector search support
    vector_index:
        type: "vector"
        similarity: "cosine"
        quantization: "scalar"
    default_args:
        host: "localhost"
        port: 27017
        username: null
        password: null
        authSource: "admin"
        database: "default"
        collection: "default"

agent:
    max_stesp: 20

imitator:
    scheduler:
        trigger: "cron"
        minute: "0,30"
        hours: "1-5"
        max_instances: 1

repos: {}
